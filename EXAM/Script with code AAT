#########################################################
# There are 4 pumking csv's
# 1. the original
# 2. deleted all the variables with missing data
# 3. as above + deleted nzv (repack) and deleted high/low price
# 4. as above + blueprint, meaning I have no variables with missing data, but I have not transformed the data. 
# 5. As above, but no lumping made, so till no variables with missing data, but I haven't lumped


#########################################################
# Split and view data based on average price as dependent variable
#########################################################

setwd("/Users/frederikboysen/Desktop/Eksamen ML/Exam2024")
pumpkin_data <- read.csv("/Users/frederikboysen/Desktop/Eksamen ML/Exam2024/US-pumpkins_4.csv", stringsAsFactors=TRUE)

view(pumpkin_data)
colSums(is.na(pumpkin_data))

### I first split the data 70/30
set.seed(123)
split <- initial_split(pumpkin_data, prop = 0.7, strata = "Average.Price")
pumpkin_train  <- training(split)
pumpkin_test   <- testing(split)

hist(pumpkin_train$Average.Price)
hist(pumpkin_test$Average.Price)

#########################################################
# KNN regression
#########################################################

# this parameter tries th k 2-25 as defines in hyper_grid variable, I'll exclude 1 since it will lead to overfitting
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Specify re-sampling strategy: k-fold cross-validation
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 1
)

knn_fit_pumpkin <- train(
  Average.Price ~ ., 
  data = pumpkin_train, 
  method = "knn", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneGrid = hyper_grid, # this parameter tries th k 2-25 as defines in hyper_grid variable
  metric = "RMSE"
)

knn_fit_pumpkin

# Plot cv-error
ggplot(knn_fit_pumpkin)

# predict average price with knn and get the test error
pred_knn = predict(knn_fit_pumpkin, newdata=pumpkin_test)
pred_knn
test_error_pumpkin = sqrt(mean((pumpkin_test$Average.Price - pred_knn)^2))
test_error_pumpkin

## COMMENT ON THE TEST ERROR AND HOW CLOSE IT IS TO THE ONE WE FOUND BASED ON THE KNN_FIT_PUMPKIN.

#########################################################
# OLS
#########################################################

# Multiple LR with OLS - using all predictores
lm.fit_pumpkin = lm(pumpkin_train$Average.Price ~ ., data=pumpkin_train)
summary(lm.fit_pumpkin)
lm.beta(lm.fit_pumpkin)
# Is there a relationship? 
  # - Look at F-statistic + its p-value and then to the individual 
  #   p-values associated with the predictors
  # - Both are highly significant evidence of a possible association

#________________
# Using cv OLS
set.seed(123)
(cv_model1 <- train(
  form = Average.Price ~ ., 
  data = pumpkin_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))

# 95%CI for the coeficients
confint(lm.fit_pumpkin, level=0.95)

 # plot data and fitted line
plot( Average.Price ~ ., pumpkin_train)    
abline(lm.fit,col='red')

# test error
pred_ols = predict(lm.fit_pumpkin, newdata=pumpkin_test)
pred_ols
test_error_OLS = sqrt(mean((pumpkin_test$Average.Price - pred_ols)^2))
test_error_OLS

#########################################################
# PCA - can only be made with continiuos variables (numeric and integer)
# Therefore I have made a dataset where i have implemented dummy encoding
#########################################################

#Checking the variables format
str(pumpkin_data) 

# Deleting 

cormatrix <- cor(pumpkin_data)
round(cormatrix, 2) 
str(pumpkin_data) 

## ____________________________________
##   Run PCA and evaluate the output
## ____________________________________

pr.out <- prcomp(pumpkin_data, scale=TRUE)
names(pr.out) #List of the 5 elements, sdev, rotation, center, scale, x
screeplot(pr.out, type="line") #displays the actual variance

pr.out$x        # PC scores for each record  
pr.out$sdev     # The std. deviations of each PC
pr.out$center   # Mean of the original variables
pr.out$scale    # Std.dev.of the original variables
pr.out$rotation # PC loading vector; 

# variance and % of variance explained by each PC (eigen values)
summary(pr.out)
# alternatively
pr.var = apply(pr.out$x, 2, var)
pve = pr.var/sum(pr.var)
pve
# Notice, PCs are ranked by how much they describe the data
# PC1 explains 6.7% of the variance in the data, 
# PC2 explains 4.4% of the variance in the data, etc..

# Represent explained variance in a graph (scree plot)
par(mfrow = c(1,2))
plot(pve, xlab="Principal Components", 
     ylab= "Proportion of variance explained",
     ylim=c(0,1), type='b') # identify the elbow of the plot and choose the 
# number of PC above the elbow
plot(cumsum(pve), xlab="Principal Component", 
     ylab="Cumulative Proportion of Variance Explained", 
     ylim=c(0,1), type='b') # check the cumulative plot and make the decision 

# Visualize the biplot
par(mfrow=c(1,1))
biplot(pr.out, scale=0) 

# See the scores on the first two principal components
pr.out$x[,1:2]


