#########################################################
# There are 4 pumking csv's
# 1. the original
# 2. deleted all the variables with missing data + made the average price variable
  # Deleting unknown variables X and X1  

  # I use The subset( ) function is the easiest way to select variables and observations.
  pumpkin_data = subset(pumpkin_data, select = -c(X, X.1))

  # Extracting the month and the year into two new columns and remove the original Date.
  pumpkin_data <- pumpkin_data %>%
    mutate(Date = mdy(Date), 
           month = month(Date),
           year = year(Date)) %>%
    select(-Date)

  # Since month and year are numeric, I will make them into factors so they are categorical
  pumpkin_data$month= factor(pumpkin_data$month)
  pumpkin_data$year= factor(pumpkin_data$year)

  #Deleting data with missing values
  #ensure the missing data are coded correctly (NA)
  pumpkin_data[pumpkin_data ==""] <- NA 

  sum(is.na(pumpkin_data))

  # Deleting the variables with a lot of missing data (Trans.Mode, Crop, Storage, Appearance, Condition, Quality, Environment, Grade)
  pumpkin_data = subset(pumpkin_data, select = -c(Trans.Mode, Crop, Storage, Appearance, Condition, Quality, Environment, Grade, Sub.Variety, Unit.of.Sale, Origin.District, Type))

  # Simple plot
  plot_missing(pumpkin_data)

  # See how many missing values there are for each column
  colSums(is.na(data))

  # Create the dependent variable: average price is (Low Price+High Price)/2
  pumpkin_data <- pumpkin_data %>%
  mutate(Average.Price = ((Low.Price + High.Price) / 2))

# 3. as above + deleted nzv (repack) and deleted high/low price

  # Check the correlation in the dataset. 
  # I do this since we use 2 variables to make our dependent variable, therefore there could be high correlation

  # Select columns 4 to 7 from your dataset, since they are the ones we need
  selected_data <- pumpkin_data[, 4:7]

  # Calculate the correlation matrix for these columns
  correlation_matrix <- cor(selected_data)

  # It can be seen that high price and low price are highly correlated and will therefore be removed. 
  pumpkin_data = subset(pumpkin_data, select = -c(High.Price, Low.Price, Mostly.High, Mostly.Low))

  # Evaluate the features with zero and near-zero variance. Decide which variables will be eliminated.
  library(caret)
  caret::nearZeroVar(data, saveMetrics = TRUE) %>% 
    tibble::rownames_to_column() %>% 
    filter(nzv)

# Delete Repack
data = subset(data, select = -Repack)

# 4. as above + blueprint, meaning I have no variables with missing data, but I have not transformed the data. 

  #Display the distributions of factor features. Decide what type of pre-processing to be implemented.
  plot_bar(data)  

  # 1. Lumping (collapsing)
  # Check the factors, and conclude if lumping is necessary 
  count(data, Origin) %>%
    arrange(n)

  count(data, Package) %>%
    arrange(n)

  # I will use lumping on these variables by stepother, where I make a variable called other. 
  # Where i will use 5% as my treshold, where all the predictors that are less than 5% will be 
  # labelled "other"

  blueprint <- recipe(Average.Price ~ ., data = data) %>%
    step_impute_knn(all_predictors(), neighbors = 6) %>% # Use KNN to impute the missing values
    step_log(all_outcomes()) %>% # Transform the dependent variable with BoxCox
    step_other(Package, threshold = 0.05, other = "other") %>% # Lumping the Package variable
    step_other(Origin, threshold = 0.05, other = "other") %>% # Lumping the Origin variable
    step_other(month, threshold = 0.05, other = "other") %>% # Lumping the month variable
    step_nzv(all_predictors()) 

# 5. As above, but no lumping made, so still no variables with missing data, but I haven't lumped. I have included high and low price in case i need these

  blueprint <- recipe(Average.Price ~ ., data = data) %>%
    step_impute_knn(all_predictors(), neighbors = 6) %>% # Use KNN to impute the missing values 

# 6. With dummy encoding

  blueprint <- recipe(Average.Price ~ ., data = data) %>%
    step_impute_knn(all_predictors(), neighbors = 6) %>% # Use KNN to impute the missing values
    step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)

#########################################################
# Split and view data based on average price as dependent variable
#########################################################

setwd("/Users/frederikboysen/Desktop/Eksamen ML/Exam2024")
pumpkin_data <- read.csv("/Users/frederikboysen/Desktop/Eksamen ML/Exam2024/US-pumpkins_4.csv", stringsAsFactors=TRUE)

view(pumpkin_data)

#Make sure the NA's are encoded correctly
pumpkin_data[pumpkin_data ==""] <- NA

# See how many datapoints without values there are for each variable
colSums(is.na(pumpkin_data))

# Drop the rows with missing values
pumpkin_data <- pumpkin_data %>% drop_na() 

### I first split the data 70/30, and i do it with strate to make sure the dependent variable is equally distributed
set.seed(123)
split <- initial_split(pumpkin_data, prop = 0.7, strata = "Average.Price")
pumpkin_train  <- training(split)
pumpkin_test   <- testing(split)

# Make sure the splittet variable has the same distribution in both train and test set
hist(pumpkin_train$Average.Price)
hist(pumpkin_test$Average.Price)

# Normalize the pricing so that you show the pricing per bushel, and not per 1 1/9 or 1/2 bushel
pumpkin_data <- pumpkin_data %>% 
  mutate(Average.Price = case_when(
    str_detect(Package, "1 1/9") ~ Average.Price/(1.1),
    str_detect(Package, "1/2") ~ Average.Price*2,
    TRUE ~ Average.Price))

# Change the name of the packages that are 1 1/9 and 1/2
pumpkin_data <- pumpkin_data %>% 
  mutate(Package = case_when(
    str_detect(Package, "1 1/9") ~ "1 bushel cartons",
    str_detect(Package, "1/2") ~ "1 bushel cartons",
    TRUE ~ Package))

# select the variables i want to use in my model
pumpkin_data <- subset(pumpkin_data, select = c(city_name, package, variety, origin, item_size, color))

# Drop rows containing missing values and encode color as factor (category)
pumpkins_data <- pumpkins_data %>% 
  drop_na() %>% 
  mutate(color = factor(color))

# Normalize the pricing so that you show the pricing per bushel, not per 1 1/9 or 1/2 bushel
pumpkin_data <- pumpkin_data %>% 
  mutate(Average.Price = case_when(
    str_detect(Package, "1 1/9") ~ Average.Price/(1.1),
    str_detect(Package, "1/2") ~ Average.Price*2,
    TRUE ~ Average.Price))

# Change the name of the packages that are 1 1/9 and 1/2
pumpkin_data <- pumpkin_data %>% 
  mutate(Package = case_when(
    str_detect(Package, "1 1/9") ~ "1 bushel cartons",
    str_detect(Package, "1/2") ~ "1 bushel cartons",
    TRUE ~ Package))

#########################################################
# KNN regression
#########################################################

# this parameter tries th k 2-25 as defines in hyper_grid variable, I'll exclude 1 since it will lead to overfitting
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Specify re-sampling strategy: k-fold cross-validation
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 1
)

knn_fit_pumpkin <- train(
  Average.Price ~ ., 
  data = pumpkin_train, 
  method = "knn", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneGrid = hyper_grid, # this parameter tries th k 2-25 as defines in hyper_grid variable
  metric = "RMSE"
)

knn_fit_pumpkin

# Plot cv-error
ggplot(knn_fit_pumpkin)

# predict average price with knn and get the test error
pred_knn = predict(knn_fit_pumpkin, newdata=pumpkin_test)
pred_knn
test_error_pumpkin = sqrt(mean((pumpkin_test$Average.Price - pred_knn)^2))
test_error_pumpkin

## COMMENT ON THE TEST ERROR AND HOW CLOSE IT IS TO THE ONE WE FOUND BASED ON THE KNN_FIT_PUMPKIN.

#########################################################
# OLS
#########################################################

# Multiple LR with OLS - using all predictores
lm.fit_pumpkin = lm(pumpkin_train$Average.Price ~ ., data=pumpkin_train)
summary(lm.fit_pumpkin)
lm.beta(lm.fit_pumpkin)
# Is there a relationship? 
  # - Look at F-statistic + its p-value and then to the individual 
  #   p-values associated with the predictors
  # - Both are highly significant evidence of a possible association

#________________
# Using cv OLS
set.seed(123)
(cv_model1 <- train(
  form = Average.Price ~ ., 
  data = pumpkin_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))

# 95%CI for the coeficients
confint(lm.fit_pumpkin, level=0.95)

 # plot data and fitted line
plot( Average.Price ~ ., pumpkin_train)    
abline(lm.fit,col='red')

# test error
pred_ols = predict(lm.fit_pumpkin, newdata=pumpkin_test)
pred_ols
test_error_OLS = sqrt(mean((pumpkin_test$Average.Price - pred_ols)^2))
test_error_OLS

#########################################################
# PCA - can only be made with continiuos variables (numeric and integer)
# Therefore I have made a dataset where i have implemented dummy encoding
#########################################################

#Checking the variables format
str(pumpkin_data) 

# Deleting 

cormatrix <- cor(pumpkin_data)
round(cormatrix, 2) 
str(pumpkin_data) 

## ____________________________________
##   Run PCA and evaluate the output
## ____________________________________

pr.out <- prcomp(pumpkin_data, scale=TRUE)
names(pr.out) #List of the 5 elements, sdev, rotation, center, scale, x
screeplot(pr.out, type="line") #displays the actual variance

pr.out$x        # PC scores for each record  
pr.out$sdev     # The std. deviations of each PC
pr.out$center   # Mean of the original variables
pr.out$scale    # Std.dev.of the original variables
pr.out$rotation # PC loading vector; 

# variance and % of variance explained by each PC (eigen values)
summary(pr.out)
# alternatively
pr.var = apply(pr.out$x, 2, var)
pve = pr.var/sum(pr.var)
pve
# Notice, PCs are ranked by how much they describe the data
# PC1 explains 6.7% of the variance in the data, 
# PC2 explains 4.4% of the variance in the data, etc..

# Represent explained variance in a graph (scree plot)
par(mfrow = c(1,2))
plot(pve, xlab="Principal Components", 
     ylab= "Proportion of variance explained",
     ylim=c(0,1), type='b') # identify the elbow of the plot and choose the 
# number of PC above the elbow
plot(cumsum(pve), xlab="Principal Component", 
     ylab="Cumulative Proportion of Variance Explained", 
     ylim=c(0,1), type='b') # check the cumulative plot and make the decision 

# Visualize the biplot
par(mfrow=c(1,1))
biplot(pr.out, scale=0) 

# See the scores on the first two principal components
pr.out$x[,1:2]

###########################################################################
# The above can also be made in the blueprint
# Method 1:  integrate PCA into the blueprint 
# using the command "step_pca(all_numeric())" (see Feature Engineering lecture)
# The command has 2 tuning parameters
#    - num_comp: # of components (default: 5)
#       e.g., step_pca(all_numeric(),-all_outcomes(), num_comp = 5)
#    - threshold: % of variance to retain (default: NA)
#       e.g., step_pca(all_numeric(),-all_outcomes(), threshold = .95)
###########################################################################

#########################################################
# PCR & PLS
#########################################################
# Method 2: specify method = "pcr" within train() in caret (see this lecture)
# In this case, we tune # of components  
#   e.g., tuneLength = 100

set.seed(12345)
cv_model_pcr <- train(
  Average.Price ~ ., 
  data = pumpkin_train, 
  method = "pcr", # see method here
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"), # an integrated method for 
  # basic data preprocessing (preProcess()) within caret
  tuneLength = 100 # see tuning parameter here
)

# The output that can be requested from the model run 
names(cv_model_pcr)

# model with lowest RMSE
cv_model_pcr$bestTune

# results for model with lowest RMSE
cv_model_pcr$results %>%
  dplyr::filter(ncomp == pull(cv_model_pcr$bestTune))

# plot cross-validated RMSE
plot(cv_model_pcr$results$RMSE)

# By controlling for multicollinearity with PCR, we experience 
# significant improvement in our predictive accuracy compared 
# to the previously obtained linear models.
# We reduce the cross-validated RMSE to nearly $ ...
# This is not a rule. Sometimes PCR can perform worse than other models. 

## Partial least squares regression (PLS)
# _________________________________________
# Similar to PCR, this technique also constructs a set of linear combinations 
# of the inputs for regression, but unlike PCR it uses the response variable  
# to aid the construction of the principal components
# PCR with caret library simply specify method = "pls" within train() 

# perform 10-fold cross validation on a PLS model tuning the 
# number of principal components to use as predictors
set.seed(234)
cv_model_pls <- train(
  Average.Price ~ ., 
  data = pumpkin_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 100
)

# model with lowest RMSE
cv_model_pls$bestTune


# results for model with lowest RMSE
cv_model_pls$results %>%
  dplyr::filter(ncomp == pull(cv_model_pls$bestTune))

#By controlling for multicollinearity with PLS, we experience 
# significant improvement in our predictive accuracy compared 
# to the previously obtained linear models 
# We reduce the cross-validated RMSE from about to nearly $ ...
# This is not a rule. Sometimes PLS can perform worse than other models. 


# plot cross-validated RMSE
plot(cv_model_pls$results$RMSE)


# ______________________________________________
# Summary of cv-error for the models tested here
# ______________________________________________
summary(resamples(list(
  model1 = cv_model_pcr, 
  model2 = cv_model_pls
)))

# ______________________
# Feature importance 
# ______________________
# For PLS and OLS, variable importance can be computed using vip library
# The importance measure is normalized from 100 (most important) to 0 (least important)
vip::vip(cv_model_pls, num_features = 20, method = "model")
vip::vip(cv_model_pcr, num_features = 20, method = "model")
# Go to the dataset description to understand the meaning of the labels (e.g.,type 
# ames in the search box and choose Raw Ames Housing Data).

# We can ask for feature importance in the OLS regression run in from R file OLS.R 
vip::vip(cv_model3, method = "model")
# notice, they are not the same features

# The command vip does not work for PCR.

# _______________________________________________
# Test error for both PCR and PLS
# ________________________________________________
pred_pcr <- predict(cv_model_pcr, pumpkin_test) 
pred_pcr 

pred_pls <- predict(cv_model_pls, pumpkin_test) 
pred_pls 
# the test RMSEA (Root mean square error of approximation)
RMSEA_pcr = sqrt(sum((pumpkin_test$Average.Price - pred_pcr)^2/529))
RMSEA_pcr

RMSEA_pls = sqrt(sum((pumpkin_test$Average.Price - pred_pls)^2/529))
RMSEA_pls

# The below gives the same as the above
test_error_pcr = sqrt(mean((pumpkin_test$Average.Price - pred_pcr)^2))
test_error_pcr

test_error_pls = sqrt(mean((pumpkin_test$Average.Price - pred_pls)^2))
test_error_pls

#########################################################
# LR, LDA & QDA
#########################################################

setwd("/Users/frederikboysen/Desktop/Eksamen ML/Exam2024")
pumpkin_data <- read.csv("/Users/frederikboysen/Desktop/Eksamen ML/Exam2024/US-pumpkins_2.csv", stringsAsFactors=TRUE)

view(pumpkin_data)

pumpkin_data[pumpkin_data ==""] <- NA

# Check the missing values in the dataset
colSums(is.na(pumpkin_data))

# select the variables i want to use in my model
pumpkin_data <- subset(pumpkin_data, select = c(City.Name, Package, Variety, Origin, Item.Size, Color, Average.Price))

# Drop rows containing missing values 
pumpkin_data <- pumpkin_data %>% drop_na() 

str(pumpkin_data)

# I will divide the average price into high and low, in order to perform LR
# I do this by finding the median price, and then use that as seperation value

median(pumpkin_data$Average.Price)
Avg.Price.bin <- ifelse(pumpkin_data$Average.Price > median(pumpkin_data$Average.Price), "High", "Low")

# Make a new dataframe with the newly created variable and split again
pumpkin_data = data.frame(pumpkin_data, Avg.Price.bin)

# Make it a factor so I can make LR on it
pumpkin_data$Avg.Price.bin <- factor(pumpkin_data$Avg.Price.bin)

# make the item size ordinal scaled
pumpkin_data$Item.Size <- factor(pumpkin_data$Item.Size, 
                                 levels = c("sml", "med", "med-lge", "lge", "xlge", "jbo", "exjbo"), 
                                 ordered = TRUE)


str(pumpkin_data)

# Display the distributions of factor features
plot_bar(pumpkin_data) 

# Check the factors, and conclude if lumping is necessary 
count(pumpkin_data, Origin) %>%
  arrange(n)

# Lump the origin variable into other for n<10
pumpkin_data$Origin <- fct_lump_min(pumpkin_data$Origin, min = 10)


# Exploring the dataset, in order to see what could have impact on price
table(is.na(pumpkin_data))
summary(pumpkin_data)
pairs(pumpkin_data)
library (GGally) 
ggpairs(pumpkin_data [, -c(9)])

# Delete the average price variable, since it can predict the high low variable 
pumpkin_data <- subset(pumpkin_data, select = -c(Average.Price))

# Split the data into train and test set
split <- initial_split(pumpkin_data, prop = 0.7, strata = "Avg.Price.bin")
pumpkin_train  <- training(split)
pumpkin_test   <- testing(split)

# This is to see what values a certain variable has
head(pumpkin_data$Item.Size)
prop.table(table(pumpkin_data$Item.Size)) # to see the distribution of the chosen variable

# multiple LR with all predictors
set.seed(123)
cv_model_lr <- train(
  Avg.Price.bin ~ ., 
  data = pumpkin_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

# Simple model
model_lr1 <- glm(Avg.Price.bin ~ ., 
              family = "binomial", 
              data = pumpkin_train)

#_________________________
# Test error
#_________________________

#_________________________
# LDA with DiscriMiner library
#_________________________

library(DiscriMiner)

da.reg <- linDA(pumpkin_data[,1:2],pumpkin_data[,3]) #In the first [] bracket put in predictors, and in the second put in dependent variable
da.reg
da.reg$functions 
da.reg$classification
da.reg$scores 
# calculate probabilities 
propensity.owner <- exp(da.reg$scores[,2])/(exp(da.reg$scores[,1])+exp(da.reg$scores[,2])) 
propensity.owner
data.frame(Actual=pumpkin_data$Avg.Price.bin,
           da.reg$classification, 
           da.reg$scores,  
           propensity.owner=propensity.owner)

#_________________________
# LDA with library MASS 
#_________________________

library(MASS)
da.reg_beta <- lda(pumpkin_data$Avg.Price.bin ~ ., data = pumpkin_data)
da.reg_beta # coefficients of linear discriminant; 
# they reflect the discriminatory power of each variable; 
# in this case, Lot_size is the most important in discriminating between classes.
# they are also used to create the LDA decision rule
# predictions
predict(da.reg_beta, pumpkin_data) # we get directly probabilities 
names(predict(da.reg_beta, pumpkin_data))
predict(da.reg_beta, pumpkin_data)$posterior #probabilities
predict(da.reg_beta, pumpkin_data)$x # linear discriminant scores
# NOTE: using the linear discriminant scores for each of the training observation
# is the second approach to classifying observations
# these values are compared against an optimum cutting score (average for each group)
# if the score is lower than the cutting score then we classify in one group
# if it is higher then it is classified in the other group
par(mar = rep(2, 4))
plot(da.reg_beta) # the plot of the linear discriminants

#########################################################
# Naive Bayes - predicting Item size used data from LR
#########################################################

library(caret)   # for NB modeling
library(e1071)   # for NB modeling

# For naive bayes all x's must be categorical
train.NB <- pumpkin_train
test.NB <- pumpkin_test

library(ggplot2)
ggplot(train.NB, aes(Item.Size)) + geom_bar()
# or
plot_bar(train.NB$Item.Size)

### NB STARTS HERE ###

library(e1071)
model.nb <- naiveBayes(Item.Size ~ ., data = train.NB)
model.nb

# Evaluating the performance of the model  
# Predict probabilities
pred.prob.nb <- predict(model.nb, newdata = test.NB, type = "raw")
pred.prob.nb

# Predict class for the Item size
pred.class.nb <- predict(model.nb, newdata = test.NB, type = "class")
pred.class.nb

# deliver the results as a dataframe
pumpkin_df_nb <- data.frame(actual = test.NB$Item.Size, predicted = pred.class.nb, pred.prob.nb)
pumpkin_df_nb

library(caret) # Confusion Matrix
confusionMatrix(pred.class.nb, test.NB$Item.Size)

# it has an accuracy of 0,44% on the test set

library(caTools) # Getting our ROC curve and AUC
colAUC(pred.prob.nb[,1], test.NB[ ,5], plotROC = TRUE) # 5 here is the column containing our y(Item.size)
