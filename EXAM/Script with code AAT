#########################################################
# There are 4 pumking csv's
# 1. the original
# 2. deleted all the variables with missing data
# 3. as above + deleted nzv (repack) and deleted high/low price
# 4. as above + blueprint, meaning I have no variables with missing data, but I have not transformed the data. 
# 5. As above, but no lumping made, so still no variables with missing data, but I haven't lumped. I have included high and low price in case i need these


#########################################################
# Split and view data based on average price as dependent variable
#########################################################

setwd("/Users/frederikboysen/Desktop/Eksamen ML/Exam2024")
pumpkin_data <- read.csv("/Users/frederikboysen/Desktop/Eksamen ML/Exam2024/US-pumpkins_4.csv", stringsAsFactors=TRUE)

view(pumpkin_data)
colSums(is.na(pumpkin_data))

### I first split the data 70/30
set.seed(123)
split <- initial_split(pumpkin_data, prop = 0.7, strata = "Average.Price")
pumpkin_train  <- training(split)
pumpkin_test   <- testing(split)

# Make sure the splittet variable has the same distribution in both train and test set
hist(pumpkin_train$Average.Price)
hist(pumpkin_test$Average.Price)

# Normalize the pricing so that you show the pricing per bushel, and not per 1 1/9 or 1/2 bushel
pumpkin_data <- pumpkin_data %>% 
  mutate(Average.Price = case_when(
    str_detect(Package, "1 1/9") ~ Average.Price/(1.1),
    str_detect(Package, "1/2") ~ Average.Price*2,
    TRUE ~ Average.Price))

# Change the name of the packages that are 1 1/9 and 1/2
pumpkin_data <- pumpkin_data %>% 
  mutate(Package = case_when(
    str_detect(Package, "1 1/9") ~ "1 bushel cartons",
    str_detect(Package, "1/2") ~ "1 bushel cartons",
    TRUE ~ Package))

# select the variables i want to use in my model
pumpkin_data <- subset(pumpkin_data, select = c(city_name, package, variety, origin, item_size, color))

# Drop rows containing missing values and encode color as factor (category)
pumpkins_select <- pumpkins_select %>% 
  drop_na() %>% 
  mutate(color = factor(color))

# Normalize the pricing so that you show the pricing per bushel, not per 1 1/9 or 1/2 bushel
pumpkin_data <- pumpkin_data %>% 
  mutate(Average.Price = case_when(
    str_detect(Package, "1 1/9") ~ Average.Price/(1.1),
    str_detect(Package, "1/2") ~ Average.Price*2,
    TRUE ~ Average.Price))

# Change the name of the packages that are 1 1/9 and 1/2
pumpkin_data <- pumpkin_data %>% 
  mutate(Package = case_when(
    str_detect(Package, "1 1/9") ~ "1 bushel cartons",
    str_detect(Package, "1/2") ~ "1 bushel cartons",
    TRUE ~ Package))

#########################################################
# KNN regression
#########################################################

# this parameter tries th k 2-25 as defines in hyper_grid variable, I'll exclude 1 since it will lead to overfitting
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Specify re-sampling strategy: k-fold cross-validation
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 1
)

knn_fit_pumpkin <- train(
  Average.Price ~ ., 
  data = pumpkin_train, 
  method = "knn", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneGrid = hyper_grid, # this parameter tries th k 2-25 as defines in hyper_grid variable
  metric = "RMSE"
)

knn_fit_pumpkin

# Plot cv-error
ggplot(knn_fit_pumpkin)

# predict average price with knn and get the test error
pred_knn = predict(knn_fit_pumpkin, newdata=pumpkin_test)
pred_knn
test_error_pumpkin = sqrt(mean((pumpkin_test$Average.Price - pred_knn)^2))
test_error_pumpkin

## COMMENT ON THE TEST ERROR AND HOW CLOSE IT IS TO THE ONE WE FOUND BASED ON THE KNN_FIT_PUMPKIN.

#########################################################
# OLS
#########################################################

# Multiple LR with OLS - using all predictores
lm.fit_pumpkin = lm(pumpkin_train$Average.Price ~ ., data=pumpkin_train)
summary(lm.fit_pumpkin)
lm.beta(lm.fit_pumpkin)
# Is there a relationship? 
  # - Look at F-statistic + its p-value and then to the individual 
  #   p-values associated with the predictors
  # - Both are highly significant evidence of a possible association

#________________
# Using cv OLS
set.seed(123)
(cv_model1 <- train(
  form = Average.Price ~ ., 
  data = pumpkin_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))

# 95%CI for the coeficients
confint(lm.fit_pumpkin, level=0.95)

 # plot data and fitted line
plot( Average.Price ~ ., pumpkin_train)    
abline(lm.fit,col='red')

# test error
pred_ols = predict(lm.fit_pumpkin, newdata=pumpkin_test)
pred_ols
test_error_OLS = sqrt(mean((pumpkin_test$Average.Price - pred_ols)^2))
test_error_OLS

#########################################################
# PCA - can only be made with continiuos variables (numeric and integer)
# Therefore I have made a dataset where i have implemented dummy encoding
#########################################################

#Checking the variables format
str(pumpkin_data) 

# Deleting 

cormatrix <- cor(pumpkin_data)
round(cormatrix, 2) 
str(pumpkin_data) 

## ____________________________________
##   Run PCA and evaluate the output
## ____________________________________

pr.out <- prcomp(pumpkin_data, scale=TRUE)
names(pr.out) #List of the 5 elements, sdev, rotation, center, scale, x
screeplot(pr.out, type="line") #displays the actual variance

pr.out$x        # PC scores for each record  
pr.out$sdev     # The std. deviations of each PC
pr.out$center   # Mean of the original variables
pr.out$scale    # Std.dev.of the original variables
pr.out$rotation # PC loading vector; 

# variance and % of variance explained by each PC (eigen values)
summary(pr.out)
# alternatively
pr.var = apply(pr.out$x, 2, var)
pve = pr.var/sum(pr.var)
pve
# Notice, PCs are ranked by how much they describe the data
# PC1 explains 6.7% of the variance in the data, 
# PC2 explains 4.4% of the variance in the data, etc..

# Represent explained variance in a graph (scree plot)
par(mfrow = c(1,2))
plot(pve, xlab="Principal Components", 
     ylab= "Proportion of variance explained",
     ylim=c(0,1), type='b') # identify the elbow of the plot and choose the 
# number of PC above the elbow
plot(cumsum(pve), xlab="Principal Component", 
     ylab="Cumulative Proportion of Variance Explained", 
     ylim=c(0,1), type='b') # check the cumulative plot and make the decision 

# Visualize the biplot
par(mfrow=c(1,1))
biplot(pr.out, scale=0) 

# See the scores on the first two principal components
pr.out$x[,1:2]

###########################################################################
# The above can also be made in the blueprint
# Method 1:  integrate PCA into the blueprint 
# using the command "step_pca(all_numeric())" (see Feature Engineering lecture)
# The command has 2 tuning parameters
#    - num_comp: # of components (default: 5)
#       e.g., step_pca(all_numeric(),-all_outcomes(), num_comp = 5)
#    - threshold: % of variance to retain (default: NA)
#       e.g., step_pca(all_numeric(),-all_outcomes(), threshold = .95)
###########################################################################

#########################################################
# PCR & PLS
#########################################################
# Method 2: specify method = "pcr" within train() in caret (see this lecture)
# In this case, we tune # of components  
#   e.g., tuneLength = 100

set.seed(12345)
cv_model_pcr <- train(
  Average.Price ~ ., 
  data = pumpkin_train, 
  method = "pcr", # see method here
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"), # an integrated method for 
  # basic data preprocessing (preProcess()) within caret
  tuneLength = 100 # see tuning parameter here
)

# The output that can be requested from the model run 
names(cv_model_pcr)

# model with lowest RMSE
cv_model_pcr$bestTune

# results for model with lowest RMSE
cv_model_pcr$results %>%
  dplyr::filter(ncomp == pull(cv_model_pcr$bestTune))

# plot cross-validated RMSE
plot(cv_model_pcr$results$RMSE)

# By controlling for multicollinearity with PCR, we experience 
# significant improvement in our predictive accuracy compared 
# to the previously obtained linear models.
# We reduce the cross-validated RMSE to nearly $ ...
# This is not a rule. Sometimes PCR can perform worse than other models. 

## Partial least squares regression (PLS)
# _________________________________________
# Similar to PCR, this technique also constructs a set of linear combinations 
# of the inputs for regression, but unlike PCR it uses the response variable  
# to aid the construction of the principal components
# PCR with caret library simply specify method = "pls" within train() 

# perform 10-fold cross validation on a PLS model tuning the 
# number of principal components to use as predictors
set.seed(234)
cv_model_pls <- train(
  Average.Price ~ ., 
  data = pumpkin_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 100
)

# model with lowest RMSE
cv_model_pls$bestTune


# results for model with lowest RMSE
cv_model_pls$results %>%
  dplyr::filter(ncomp == pull(cv_model_pls$bestTune))

#By controlling for multicollinearity with PLS, we experience 
# significant improvement in our predictive accuracy compared 
# to the previously obtained linear models 
# We reduce the cross-validated RMSE from about to nearly $ ...
# This is not a rule. Sometimes PLS can perform worse than other models. 


# plot cross-validated RMSE
plot(cv_model_pls$results$RMSE)


# ______________________________________________
# Summary of cv-error for the models tested here
# ______________________________________________
summary(resamples(list(
  model1 = cv_model_pcr, 
  model2 = cv_model_pls
)))


# _____________________________________________________________________
# Summary of of cv-error for all models tested here and in R file OLS.R
# _____________________________________________________________________
# ! to execute the following lines, first run the code from R file OLS.R
summary(resamples(list(
  model1 = cv_model1, 
  model2 = cv_model2, 
  model3 = cv_model3,
  model4 = mod_wo_Garage_Cars,
  model5 = mod_wo_Garage_Area,
  model6 = model_withFE,
  model7 = cv_model_pcr,
  model8 = cv_model_pls
)))
# compare the models based on the output and conclude on the best model/algorithm


# ______________________
# Feature importance 
# ______________________
# For PLS and OLS, variable importance can be computed using vip library
# The importance measure is normalized from 100 (most important) to 0 (least important)
vip::vip(cv_model_pls, num_features = 20, method = "model")
vip::vip(cv_model_pcr, num_features = 20, method = "model")
# Go to the dataset description to understand the meaning of the labels (e.g.,type 
# ames in the search box and choose Raw Ames Housing Data).

# We can ask for feature importance in the OLS regression run in from R file OLS.R 
vip::vip(cv_model3, method = "model")
# notice, they are not the same features

# The command vip does not work for PCR.

# _______________________________________________
# Test error for both PCR and PLS
# ________________________________________________
pred_pcr <- predict(cv_model_pcr, pumpkin_test) 
pred_pcr 

pred_pls <- predict(cv_model_pls, pumpkin_test) 
pred_pls 
# the test RMSEA (Root mean square error of approximation)
RMSEA_pcr = sqrt(sum((pumpkin_test$Average.Price - pred_pcr)^2/529))
RMSEA_pcr

RMSEA_pls = sqrt(sum((pumpkin_test$Average.Price - pred_pls)^2/529))
RMSEA_pls

# The below gives the same as the above
test_error_pcr = sqrt(mean((pumpkin_test$Average.Price - pred_pcr)^2))
test_error_pcr

test_error_pls = sqrt(mean((pumpkin_test$Average.Price - pred_pls)^2))
test_error_pls
