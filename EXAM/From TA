# 1. Set the working directory
setwd("/Users/frederikboysen/Desktop/Machine Learning/Dataset")

# 2.  Import the dataset
dataset <- read.table("WestRoxbury.csv",sep=',', header = TRUE)

# 2. problem
# 1. inspecting data type
dim(WestRoxbury)
class(WestRoxbury)
head(WestRoxbury)

# 2. Show the first 10 rows of the first column only
dataset[1:10, 1:1]

# 3. Show the first 10 rows of each of the columns
dataset[1:10, ]

# 4. Show the fifth row of the first 10 columns
dataset[5, 1:10]

# 5. Show the whole first column
dataset[, 1]

# 6. Find the mean of the first column
mean(dataset[, 1])

# 7. Find summary statistics for each column
summary(dataset)

#8. Check the type of variables using the command str(). Which of the variables are categorical variables?
str(dataset)

#9. Convert the categorical variables to factors and identify the levels of the factor variables using the commands
dataset$REMODEL= as.factor(dataset$REMODEL)  
levels(dataset$REMODEL)

# 10. Export your new data using the command
write.csv(dataset, "Tutorial 1.csv")

# Problem 3: Programming

# 1. Define v1 <- c(1, 2, 2, 1) and v2 <- c(2, 3, 3, 2).
v1 <- c(1,2,2,1)
v2 <- c(2,3,3,2)

v1 + v2
v1 - v2
v1 * v2

v3 <- c(v1, v2)

# Define a 2 x 3 matrix
mA <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)

# (a) Print the minimum and the maximum of each row.
min(mA[1, ])
min(mA[2, ])

max(mA[1, ])
max(mA[2, ])

# (b) Compute the sum of each column and print it as a vector.
sv1 <- sum(mA[,1])
sv2 <- sum(mA[,2])
sv3 <- sum(mA[,3])

vector <- c(sv1, sv2, sv3)
print(vector)

# (c) Sort in ascending order the elements on the first column of mA
sorted_matrix <- sort(mA[,1])

# 3. Consider the function y = f(x)
fx <- function(x){
  if (x <= 0) {
    y = -x^3
    } else if (x> 0 && x< 1) {
      y = x^2 
    } else if (x > 1) {
  y = sqrt(x)
    }
  return(y)
}

fx(-2)
fx(0.5)
fx(4)

# 4. Let h(x, n)
h_xn <- function(x,n) {
  h = 0
  for (i in 0:n) {
  h = h + x^i
  } 
  return(h)
}

h_xn(3,4)

# 5. While loop
h1_xn <- function(x,n) {
  h = 0
  i = 0
  while(i<=n) {
    h = h + x^i
    i = i + 1
  }
  return(h)
}

h1_xn(3,4)

# 3.6
mB1 <- matrix(1,4,4)
mB2 <- diag(1, 4, 4)
print(mB1)
print(mB)

mB3 <- mB1 - mB2
print(mB3)


#############################################################
# TA 2
#############################################################

car_dataset <- read.table("2. tutorial.csv",sep=',', header = TRUE)

dim(car_dataset)
class(car_dataset)
head(car_dataset)

# 2 setting the variables as described in the text
car_dataset$Make= factor(car_dataset$Make)

class(car_dataset$Make) # checking if it's the right class, can be done for all variables 

car_dataset$Make= factor(car_dataset$Make)
car_dataset$Year= factor(car_dataset$Year)
car_dataset$Engine.Fuel.Type= factor(car_dataset$Engine.Fuel.Type)
car_dataset$Transmission.Type= factor(car_dataset$Transmission.Type)
car_dataset$Driven_Wheels= factor(car_dataset$Driven_Wheels)
car_dataset$Market.Category= factor(car_dataset$Market.Category)
car_dataset$Vehicle.Size= factor(car_dataset$Vehicle.Size)
car_dataset$Vehicle.Style= factor(car_dataset$Vehicle.Style)

class(car_dataset$MSRP)

sapply(car_dataset, class)

summary(car_dataset[c("Engine.HP","Engine.Cylinders","Transmission.Type")])

# 3. Delete a variable - I use The subset( ) function is the easiest way to select variables and observations.
car_dataset = subset(car_dataset, select = -Model)

#4. Set a seed and randomly partition the data into training and test set (70%/30%)
# Set.seed chooses the first observation/ row in the dataset that we use to train the model
set.seed(123) 
library(rsample) # I'm using the rsample package
car_split <- initial_split(car_dataset, prop = 0.7, strata = "MSRP")
car_train  <- training(car_split)
car_test   <- testing(car_split)

# 5. Evaluate MSRP distribution and check various transformations to normalize it
hist(car_dataset$MSRP)
hist(car_train$MSRP)
hist(car_test$MSRP)

library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks

transform1 <- log(car_dataset$MSRP) 
transform2 <- log(car_train$MSRP)

hist(transform1)
hist(transform2)

library(forecast)

Transform_boxcox <- forecast::BoxCox(car_train$MSRP, lambda = "auto")
hist (Transform_boxcox)

# 6. Evaluate the missing data. Decide which method of treating missing data will be applied.
sum(is.na(car_train))

plot_missing(car_train)

car_train %>%
  is.na() %>%
  reshape2::melt() %>%
  ggplot(aes(Var2, Var1, fill=value)) + 
    geom_raster() + 
    coord_flip() +
    scale_y_continuous(NULL, expand = c(0, 0)) +
    scale_fill_grey(name = "", 
                    labels = c("Present", 
                               "Missing")) +
    xlab("Observation") +
    theme(axis.text.y  = element_text(size = 4))

library (Hmisc) 
car_train$Engine.HP <- impute(car_train$Engine.HP, fun = median)
car_train$Number.of.Doors <- impute(car_train$Number.of.Doors, fun = median)
car_train$Engine.Cylinders <- impute(car_train$Engine.Cylinders, fun = median)

# 7. Evaluate the features with zero and near-zero variance. Decide which variables will be eliminated.

library(caret)
caret::nearZeroVar(car_train, saveMetrics = TRUE) %>% 
  tibble::rownames_to_column() %>% 
  filter(nzv)


# 8. Display the distributions of the numeric features. Decide what type of pre-processing to be implemented later.

library(DataExplorer)
plot_histogram(car_train) 

# Normalize all numeric columns using the blueprint
car_recipe %>% step_YeoJohnson(all_numeric())  
# Standardize all numeric values using the blueprint
data_recipe %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes())

# 9.  Display the distributions of factor features. Decide what type of pre-processing to beimplemented later.
plot_bar(car_train)  

count(car_train, Make) %>% arrange(n)
count(car_train, Year) %>% arrange(n)
count(car_train, Engine.Fuel.Type) %>% arrange(n)
count(car_train, Transmission.Type) %>% arrange(n)
count(car_train, Driven_Wheels) %>% arrange(n)
count(car_train, Market.Category) %>% arrange(n)

# 10. Based on the data exploration above, proceed by creating a blueprint that prepares the
# data for predicting MSRP, using the recipes package.

blueprint <- recipe(MSRP ~ ., data = car_train) %>%
  step_impute_knn(all_predictors(), neighbors = 6) %>%
  step_log(all_outcomes()) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_other(Make, threshold = 0.01, other = "other") %>%
  step_other(Engine.Fuel.Type, threshold = 0.01, other = "other") %>%
  step_other(Transmission.Type, threshold = 0.01, other = "other") %>%
  step_other(Vehicle.Style, threshold = 0.01, other = "other") %>%
  step_nzv(all_predictors()) %>%
  step_zv() 

blueprint

prepare <- prep(blueprint, training = car_train)
prepare
# bake: apply the recipe to new data (e.g., the training data or future test data) with bake()
baked_train <- bake(prepare, new_data = car_train)
baked_test <- bake(prepare, new_data = car_test)
baked_train
baked_test

cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

knn_fit2 <- train(
  blueprint, 
  data = car_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)

knn_fit2

#############################################################
# TA 3
#############################################################

library(MASS) 
library(ISLR2)
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
library(rsample)
library(caret)    # for cross-validation, etc.
library(vip)      # variable importance
library(GGally) 
```


Explore the dataset
```{r Boston}
head(Boston)

Boston <- ISLR2::Boston

view(Boston)
summary(Boston)
plot_missing(Boston)
plot_histogram(Boston)
hist(Boston$crim)
str(Boston)
ggpairs(Boston,columns = 1:10)

# Look at correlation
```

Model an ordinary least square regression (lm) with crim as a dependent variable and
all the remaining variables as predictors (without data engineering) and using k-fold cv
technique.
```{r Problem 1.2}
set.seed(123) 
boston_split <- initial_split(Boston, prop = 0.8, strata = "crim")
boston_train <- training(boston_split)
boston_test  <- testing(boston_split)

set.seed(123)
cv_model <- train(
  crim ~ ., 
  data = boston_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)

cv_model
cv_model$finalModel 

summary(cv_model)
```

Check if the OLS assumptions are fulfilled (as done in the classical approach)

```{r}
par(mfrow=c(2,2))
plot(cv_model$finalModel)
# Only use this in order to visualize assumptions

#Residuals vs fitted plot - linearity and homoskedacity we are looking at a pattern, if there's a pattern it's a problem. We want a horizontal line where the residuals are formed equally

#Scale-location - We want the same with this one as above.

#Normality plot/ Normal Q-Q - all the dots should preferably be on the line. 

# Residuals vs Leverage - The plot where we see outliers. How problematic are the outliers. 

# OLS assumptions is only for linear models. 
```


```{r Problem 1.3}

# (1) Linear regression assumes a linear relationship between the predictor(s) 
# and the response variable
Boston1 <- broom::augment(cv_model$finalModel, data = boston_train)
p1 <- ggplot(Boston1, aes(age, crim)) + 
  geom_point(size = 1, alpha = .4) +
  geom_smooth(se = FALSE) +
  scale_y_continuous("crim", labels = scales::comma) +
  xlab("Age") +
  ggtitle(paste("Non-transformed variables with a\n",
                "non-linear relationship."))
p1

# (2). Constant variance among residuals

Boston2 <- broom::augment(cv_model$finalModel, data = boston_train)
p2 <- ggplot(Boston2, aes(.fitted, .std.resid)) + 
  geom_point(size = 1, alpha = .4)  +
  xlab("Predicted values") +
  ylab("Residuals") +
  ggtitle("Model 2", subtitle = "crim ~ .")

gridExtra::grid.arrange(p1, p2, nrow = 1)

# (3) Linear regression assumes the errors are independent (uncorrelated)

Boston3 <- mutate(Boston2, id = row_number())

p3 <- ggplot(Boston3, aes(id, .std.resid)) + 
  geom_point(size = 1, alpha = .4) +
  xlab("Row ID") +
  ylab("Residuals") +
  ggtitle("Model 3", subtitle = "Less correlated residuals")

gridExtra::grid.arrange(p1, p2, nrow = 1)

# (4) Multicollinearity
cormatrix <- cor(Boston)
round(cormatrix,2)

summary(cv_model) %>%
  broom::tidy() %>%
  filter(term %in% c("rad", "tax"))


```

```{r}
# test model again without rad
library(dplyr)
set.seed(234)
mod_wo_rad <- train(
  crim ~ ., 
  data = dplyr::select(boston_train, -rad), 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)

summary(mod_wo_rad) %>%
  broom::tidy() %>%
  filter(term == "tax")

# test model without tax
set.seed(123)
mod_wo_tax <- train(
  crim ~ ., 
  data = dplyr::select(boston_train, -tax), 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)

summary(mod_wo_tax) %>%
  broom::tidy() %>%
  filter(term == "rad")

```


4. Next, consider the same model applying feature engineering and using a blueprint.
```{r Problem 1.4}

attach(Boston)
    par(mfrow=c(1,3))
    hist(boston_train$crim, breaks = 20, 
         col = "red", border = "red", 
         ylim = c(0, 400))
   
# log if all value are positive
transformed_response <- log(boston_train$crim)
    
# Box-Cox transformation (lambda=0 is equivalent to log(x))
library(forecast)
transformed_response_BC <- forecast::BoxCox(boston_train$crim, 
                                                lambda="auto") 
    
hist(transformed_response, breaks = 20, 
  col = "lightgreen", border = "lightgreen",
  ylim = c(0, 75) )
hist(transformed_response_BC, breaks = 20, 
  col = "lightblue", border = "lightblue", 
  ylim = c(0, 75))

```

```{r Blueprint}
boston_recipe <- recipe(crim ~ ., data = boston_train)  %>%
  step_log(all_outcomes()) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) 

boston_recipe
# prepare
prepare <- prep(boston_recipe, training = boston_train)
prepare
# bake
baked_train <- bake(prepare, new_data = boston_train)
baked_test <- bake(prepare, new_data = boston_test)
baked_train
baked_test

# Implement some feature engineering and re-train the model
set.seed(123)
model_withFE <- train(
  crim ~ ., 
  data = baked_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)

```
5. Repeat for KNN regression, PCR regression and PLS regression.
```{r Problem 1.5}
#KNN
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 1
)

hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

knn_fit2 <- train(
  boston_recipe, 
  data = boston_train, 
  method = "knn", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)

knn_fit2
plot(knn_fit2)

```

```{r PCR}
#PCR
set.seed(123)
cv_model_pcr <- train(
  crim ~ ., 
  data = boston_train, 
  method = "pcr", # see method here
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"), # an integrated method for 
  # basic data preprocessing (preProcess()) within caret
  tuneLength = 100 # see tuning parameter here
)

# model with lowest RMSE
cv_model_pcr$bestTune

# results for model with lowest RMSE
cv_model_pcr$results %>%
  dplyr::filter(ncomp == pull(cv_model_pcr$bestTune))

# plot cross-validated RMSE
plot(cv_model_pcr$results$RMSE)
```

```{r PLS}
set.seed(234)
cv_model_pls <- train(
  crim ~ ., 
  data = boston_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 100
)

# model with lowest RMSE
cv_model_pls$bestTune


# results for model with lowest RMSE
cv_model_pls$results %>%
  dplyr::filter(ncomp == pull(cv_model_pls$bestTune))

#By controlling for multicollinearity with PLS, we experience 
# significant improvement in our predictive accuracy compared 
# to the previously obtained linear models 
# We reduce the cross-validated RMSE from about to nearly $ ...
# This is not a rule. Sometimes PLS can perform worse than other models. 


# plot cross-validated RMSE
plot(cv_model_pls$results$RMSE)
```
6. Evaluate the models’ prediction performance criteria and compare the results (e.g., RMSE)
using a list in R.
```{r}
summary(resamples(list(
  model1 = cv_model, 
  model2 = mod_wo_rad,
  model3 = mod_wo_tax,
  model4 = model_withFE,
  model5 = cv_model_pcr,
  model6 = cv_model_pls,
  model7 = knn_fit2
)))

predictions_knn_train <- predict(knn_fit2, boston_train)
train_RMSE_knn =sqrt(mean((log(boston_train$crim) - predictions_knn_train)^2))
train_RMSE_knn

```
7. Detect the variables’ importance.
```{r}
vip::vip(cv_model, method = "model")
vip::vip(mod_wo_rad, method = "model")
vip::vip(mod_wo_tax, method = "model")
vip::vip(model_withFE, method = "model")
vip::vip(cv_model_pcr, method = "model")
vip::vip(cv_model_pls, method = "model")
vip::vip(knn_fit2, method = "model")

```

8. Evaluate the OLS assumptions and compare them with your results in 3.
```{r}
par(mfrow=c(2,2))
plot(model_withFE$finalModel)

# We can observe a general improvement in all aspects of the model’s performance, 
# including the residual versus fitted values plot and the Normal Q-Q plot
